{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised linear models\n",
    "\n",
    "In this notebook, we're going to examine generalised linear models with polynomial coefficients using a toy dataset. We'll look at examples of overfitting and underfitting, and how these can be combatted using **regularisation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate toy data\n",
    "We're using 30 samples from a 1D cosine function with Gaussian noise added.\n",
    "\n",
    "What we'd like is for our regression model to learn the generating function from these 30 samples, without overfitting to the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_function(x):\n",
    "    return np.cos(1.5 * np.pi * x)\n",
    "\n",
    "num_samples = 30\n",
    "\n",
    "X = np.sort(np.random.rand(num_samples))\n",
    "y = generating_function(X) + np.random.randn(num_samples) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise: Print a sample of X and y so you can see what the inputs and corresponding outputs look like.**\n",
    "\n",
    "** Exercise: Plot a graph of the generating function with X as the domain. Plot the (X, y) samples on the same graph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building linear models\n",
    "\n",
    "**Exercise: Fit a linear regression model using a polynomial of degree 2. Hint: Check out the sklearn documentation on PolynomialFeatures and LinearRegression.**\n",
    "\n",
    "**Exercise: With the model you've built, use the code below to plot a graph showing how the model looks compared to the actual generating function (over an artificial test set).**\n",
    "\n",
    "```\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "plt.plot(X_test, model.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "plt.plot(X_test, generating_function(X_test), label=\"Generating function\")\n",
    "plt.scatter(X, y, label=\"Samples\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Exercise: Print the coefficients from your model. What do these coefficients mean?**\n",
    "\n",
    "**Exercise: Run the above code again but now with the degree set to 4, and then 15. What do you notice about the graph and coefficients?**\n",
    "\n",
    "**Exercise: Let's see if we can fix the problem with the degree-15 model without decreasing the degree itself. Build Lasso, Ridge and ElasticNet models with the degree still set to 15. Adjust the parameters of these models until you can produce at least one graph that looks good.**\n",
    "\n",
    "**Exercise: Imagine that you've built the perfect model and now you want to hand it over to an engineer so that they can use it to make predictions. Is it possible to save this model so that it can be reused later without needing sklearn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
