{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "\n",
    "What we'd like to do is learn how to classify each document into the correct newsgroup based only on the text in the document. To keep things simple, we just use 4 newsgroups: `'alt.atheism', 'soc.religion.christian', 'comp.graphics' and 'sci.med'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "The following code will download training and test sets containing the documents. It might take a little bit of time to fetch the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=0)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=categories,\n",
    "                                 shuffle=True,\n",
    "                                 random_state=0)\n",
    "\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(twenty_train.target))\n",
    "print(\"First target is\", twenty_train.target_names[twenty_train.target[0]])\n",
    "print(\"First document is\", twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "For each training document, we count the number of occurrences of each word (*term*) and use this to build a **term-document** matrix. This matrix contains the frequency of terms that occur in the set of training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(twenty_train.data)\n",
    "print(X_train_counts.shape)\n",
    "print(type(X_train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts[0, 26895])\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "Let's now fit a naive Bayes model to the training data. This model will learn the mapping between the document term frequencies and the newsgroup the document was posted in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['Reading the Bible',\n",
    "            'OpenGL vs DirectX',\n",
    "            'Diabetes and glucose',\n",
    "            'Humanists in the Republican Party']\n",
    "X_new_counts = count_vectorizer.transform(docs_new)\n",
    "\n",
    "predicted = model.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{0} => {1}'.format(doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = twenty_test.data\n",
    "X_new_counts = count_vectorizer.transform(docs_test)\n",
    "predicted = model.predict(X_new_counts)\n",
    "print(\"Test set accuracy is\", np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(twenty_test.target,\n",
    "                            predicted,\n",
    "                            target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
