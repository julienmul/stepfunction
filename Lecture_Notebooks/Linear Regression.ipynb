{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np     # matrix structures, linear algebra functions etc\n",
    "import matplotlib.pyplot as plt    # plotting library\n",
    "from sklearn.linear_model import LinearRegression   # linear regression model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error   # accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate toy data\n",
    "Given a sample of 30 data points lying approximately on the straight line $y = 2x - 3$, we'd like to learn the equation of the line from the data alone.\n",
    "\n",
    "We call the straight line the **generating function** -- it is the fundamental relationship that generates outputs $y$ from inputs $x$.\n",
    "\n",
    "In a situation with real data, you would not already know what the generating function is and usually it would be much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_function(x):\n",
    "    return 2 * x - 3\n",
    "\n",
    "num_samples = 30\n",
    "\n",
    "X_training = np.sort(10 * np.random.rand(num_samples))\n",
    "y_training = generating_function(X_training) + np.random.randn(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a few samples from the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_training[:10], y_training[:10]):\n",
    "    print(\"input = {0:.3f}, output = {1:.3f}\".format(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot these (noisy) samples together with the actual generating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_training, generating_function(X_training), 'green', label='Generating Function')\n",
    "plt.scatter(X_training, y_training, label='Training Points')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "We could learn a \"wiggly\" model that fits the training points exactly. But this wouldn't generalise well to new data.\n",
    "\n",
    "This is the difference between learning the **generating function** underlying the data and **overfitting** to the noise.\n",
    "\n",
    "To avoid overfitting, we try to learn the **simplest possible** model that fits the data reasonably accurately.\n",
    "\n",
    "Usually, you will need to trade off **model complexity** against **accuracy** over the training set to get the best performance over the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "# Note that we can't just use X itself as sklearn no longer allows 1D arrays here,\n",
    "# so we change it to a 2D array\n",
    "model.fit(X_training[:, np.newaxis], y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the model\n",
    "\n",
    "Let's now plot a graph showing how the model looks compared to the actual generating function over a test set.\n",
    "\n",
    "Remember: this model has been learnt entirely from the training points alone with no access to the actual generating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(0, 10, 100)\n",
    "y_test = model.predict(X_test[:, np.newaxis])\n",
    "y_actual = generating_function(X_test)\n",
    "plt.plot(X_test, y_test, label=\"Model\")\n",
    "plt.plot(X_test, y_actual, label=\"Generating Function\")\n",
    "plt.scatter(X_training, y_training, label=\"Training Points\")\n",
    "plt.xlim((0, 10))\n",
    "plt.ylim((-3, 15))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the coefficients of the linear model. Remember that the generating function's equation was $y = 2x - 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Although the plot looks promising, it would be good to obtain a single number representing the performance of the model. This will also allow us to compare different models against each other in a principled way.\n",
    "\n",
    "Let's first look at the **mean absolute error** (MAE). This is simply the sum of the differences between the actual value of $y$ and the predicted value of $y$ divided by the total number of data points. In other words, it is the average value of the errors over all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE over training set\n",
    "y_training_pred = model.predict(X_training[:, np.newaxis])\n",
    "mean_absolute_error(y_training, y_training_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE over test set\n",
    "mean_absolute_error(y_actual, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the **mean-squared error** (MSE). This is similar to MAE, except that we take the average of the **squared** differences of the errors.\n",
    "\n",
    "Note that the linear regression training procedure works by trying to minimise the MSE over the training set, so we would hope this is quite low at least for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE over training set\n",
    "mean_squared_error(y_training, y_training_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE over test set\n",
    "mean_squared_error(y_actual, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, any sklearn model has its own built-in `score()` method providing a default evaluation criterion for the specific type of problem it is meant to solve. In the case of `LinearRegression`, this is the **coefficient of determination** $R^2$.\n",
    "\n",
    "Roughly speaking, $R^2$ measures deviation from the \"identity line\" of perfect prediction.\n",
    "\n",
    "The best possible value of $R^2$ is 1.0 (meaning perfect prediction) and at worst it can be negative (meaning the model is extremely poor!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training R^2\n",
    "print(model.score(X_training[:, np.newaxis], y_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R^2\n",
    "print(model.score(X_test[:, np.newaxis], y_actual))\n",
    "\n",
    "plt.scatter(y_test, y_actual, marker='.')\n",
    "plt.xlabel('Predicted y')\n",
    "plt.ylabel('Actual y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
