{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "In this classification task, we'd like to distinguish 3 different species (Setosa, Versicolour, and Virginica) of iris flowers based on their petal and sepal length and width.\n",
    "\n",
    "This data set is built into sklearn, so it's straightforward to load it in. See [here](https://en.wikipedia.org/wiki/Iris_flower_data_set) for more details on the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "y_actual = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Targets are ['setosa' 'versicolor' 'virginica']\n",
      "Training set size is 105\n",
      "Test set size is 45\n",
      "Input = [ 5.   2.   3.5  1. ]; output = 1 (species = versicolor)\n",
      "Input = [ 6.5  3.   5.5  1.8]; output = 2 (species = virginica)\n",
      "Input = [ 6.7  3.3  5.7  2.5]; output = 2 (species = virginica)\n",
      "Input = [ 6.   2.2  5.   1.5]; output = 2 (species = virginica)\n",
      "Input = [ 6.7  2.5  5.8  1.8]; output = 2 (species = virginica)\n",
      "Input = [ 5.6  2.5  3.9  1.1]; output = 1 (species = versicolor)\n",
      "Input = [ 7.7  3.   6.1  2.3]; output = 2 (species = virginica)\n",
      "Input = [ 6.3  3.3  4.7  1.6]; output = 1 (species = versicolor)\n",
      "Input = [ 5.5  2.4  3.8  1.1]; output = 1 (species = versicolor)\n",
      "Input = [ 6.3  2.7  4.9  1.8]; output = 2 (species = virginica)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features are\", iris_data.feature_names)\n",
    "print(\"Targets are\", iris_data.target_names)\n",
    "print(\"Training set size is\", len(X_train))\n",
    "print(\"Test set size is\", len(X_test))\n",
    "for a, b in zip(X_train[:10], y_train[:10]):\n",
    "    print(\"Input = {0}; output = {1} (species = {2})\".format(a, b,\n",
    "                                                             iris_data.target_names[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and output the model\n",
    "In the next two steps, we build the decision tree model from the training set and export it to a file for viewing in GraphViz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(model,\n",
    "                             out_file=f,\n",
    "                             feature_names=iris_data.feature_names,  \n",
    "                             class_names=iris_data.target_names,  \n",
    "                             filled=True,\n",
    "                             rounded=True,  \n",
    "                             special_characters=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Let's first print the confusion matrix as we usually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the **precision**, **recall** and **$F_1$ score** for each class.\n",
    "\n",
    "**Example**: For the \"versicolor\" class, the precision is equal to the proportion of irises predicted to be \"versicolor\" that were indeed \"versicolor\".\n",
    "\n",
    "The recall is equal to the proportion of irises that are in fact \"versicolor\" that the classifier correctly predicted to be \"versicolor\".\n",
    "\n",
    "If our classifier hypothetically labelled everything as \"versicolor\", this would give us a low precision and high recall (100%) for this class.\n",
    "\n",
    "If our classifier labelled only a single iris (where it was absolutely sure of its prediction) as \"versicolor\", this would give us a high precision (100%) and low recall for this class.\n",
    "\n",
    "Typically, we have to trade off precision against recall based on what is most important for our problem.\n",
    "\n",
    "The $F_1$ score is equal to the harmonic mean of precision and recall. In other words, it gives equal weight to the precision and recall and then computes their average to give us a single score for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        16\n",
      " versicolor       1.00      0.94      0.97        18\n",
      "  virginica       0.92      1.00      0.96        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual,\n",
    "                            y_pred,\n",
    "                            target_names=iris_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
